{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ **This notebook has been updated in this repository [here](https://github.com/jhj0517/document_classification)!**\n",
        "\n",
        "ðŸ–‹ **Author**: [jhj0517](https://github.com/jhj0517/document_classification/tree/master/notebook/document_classification.ipynb)"
      ],
      "metadata": {
        "id": "ZNDh3Ac_xWbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Check GPU\n",
        "#@markdown To train the model, you should enable GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "mDHajf1Ba6nr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Installation\n",
        "#@markdown This cell will install dependencies for training\n",
        "!git clone https://github.com/jhj0517/document_classification.git\n",
        "%cd document_classification\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "doxAWSOFZQVg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Mount your Gdrive\n",
        "#@markdown The model file should be saved in your Gdrive path.\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "Ml1Z3cSqbPtu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Configure arguments\n",
        "#@markdown This section is used to configure some path arguments.\n",
        "\n",
        "#@markdown The dataset must have \"document\" column and \"label\" column like this.\n",
        "\n",
        "#@markdown | Label    | Document       |\n",
        "#@markdown |----------|----------------|\n",
        "#@markdown | sadness   | I'm so sad  |\n",
        "#@markdown | happiness  | I'm so happy  |\n",
        "\n",
        "#@markdown See [example dataset](https://github.com/jhj0517/document_classification/tree/master/example_data) to see what the actual dataset looks like.\n",
        "\n",
        "DATA_PATH = '/content/document_classification/example_data/example_dataset.xlsx' #@param {type: \"string\"}\n",
        "MODEL_PATH = '/gdrive/MyDrive/document_classification' #@param {type: \"string\"}\n",
        "\n",
        "arguments = \"\"\n",
        "if DATA_PATH:\n",
        "  arguments += f\" --data_path {DATA_PATH}\"\n",
        "if MODEL_PATH:\n",
        "  arguments += f\" --model_path {MODEL_PATH}\""
      ],
      "metadata": {
        "id": "pInXDaBFVy__",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Configure training arguments (Optional)\n",
        "#@markdown This section is used to configure some training arguments. You can pass this section if you don't know about it.\n",
        "batch_size = 32 #@param {type: \"integer\"}\n",
        "learning_rate = 5e-5 #@param {type: \"number\"}\n",
        "max_seq_length = 128 #@param {type: \"integer\"}\n",
        "epochs = 6 #@param {type: \"integer\"}\n",
        "seed = 7 #@param {type: \"integer\"}\n",
        "\n",
        "if batch_size:\n",
        "  arguments += f\" --batch_size {batch_size}\"\n",
        "if learning_rate:\n",
        "  arguments += f\" --learning_rate {learning_rate}\"\n",
        "if max_seq_length:\n",
        "  arguments += f\" --max_seq_length {max_seq_length}\"\n",
        "if epochs:\n",
        "  arguments += f\" --epochs {epochs}\"\n",
        "if seed:\n",
        "  arguments += f\" --seed {seed}\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "8WU3ufLF6kpw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Train\n",
        "#@markdown This section begins the training.\n",
        "if 'arguments' in locals():\n",
        "  print(f'training starts with arguments: {arguments}')\n",
        "  !python train.py {arguments}\n",
        "else:\n",
        "    !python train.py"
      ],
      "metadata": {
        "id": "HaSXJR2_ZyFh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Test model\n",
        "#@markdown Test your model here with any text input.\n",
        "\n",
        "from ratsnlp.nlpbook.classification import ClassificationDeployArguments\n",
        "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "import pandas as pd\n",
        "import argparse\n",
        "\n",
        "DATA_PATH = '/content/document_classification/example_data/example_dataset.xlsx' #@param {type: \"string\"}\n",
        "MODEL_PATH = '/gdrive/MyDrive/document_classification' #@param {type: \"string\"}\n",
        "INPUT = \"I'm surprised\" #@param {type: \"string\"}\n",
        "\n",
        "class DataSet:\n",
        "    def __init__(self):\n",
        "        self.data_path = DATA_PATH\n",
        "        self.df = pd.read_excel(self.data_path)\n",
        "        pass\n",
        "\n",
        "    def get_labels(self):\n",
        "        excel_data_df = self.df\n",
        "        intents = excel_data_df['label']\n",
        "        return list(intents.unique())\n",
        "\n",
        "    @property\n",
        "    def num_labels(self):\n",
        "        return len(self.get_labels())\n",
        "\n",
        "class ClassificationModel:\n",
        "    def __init__(self, data_set):\n",
        "        self.data_set = data_set\n",
        "        self.args = ClassificationDeployArguments(\n",
        "            pretrained_model_name=\"beomi/kcbert-base\",\n",
        "            downstream_model_dir=MODEL_PATH,\n",
        "            max_seq_length=128,\n",
        "        )\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\n",
        "            self.args.pretrained_model_name,\n",
        "            do_lower_case=False,\n",
        "        )\n",
        "        self.fine_tuned_model_ckpt = torch.load(\n",
        "            self.args.downstream_model_checkpoint_fpath,\n",
        "            map_location=torch.device(\"cpu\")\n",
        "        )\n",
        "        self.pretrained_model_config = BertConfig.from_pretrained(\n",
        "            self.args.pretrained_model_name,\n",
        "            num_labels=self.fine_tuned_model_ckpt['state_dict']['model.classifier.bias'].shape.numel(),\n",
        "        )\n",
        "        self.model = BertForSequenceClassification(self.pretrained_model_config)\n",
        "        self.activate = False\n",
        "\n",
        "    def activate_model(self):\n",
        "        if not self.activate:\n",
        "            self.activate = True\n",
        "            self.model.load_state_dict(\n",
        "                {k.replace(\"model.\", \"\"): v for k, v in self.fine_tuned_model_ckpt['state_dict'].items()})\n",
        "            self.model.eval()\n",
        "\n",
        "    def inference(self, sentence):\n",
        "        self.activate_model()\n",
        "        label_names = self.data_set.get_labels()\n",
        "        # Preprocess the input\n",
        "        inputs = self.tokenizer(\n",
        "            [sentence],\n",
        "            max_length=self.args.max_seq_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**{k: torch.tensor(v) for k, v in inputs.items()})\n",
        "\n",
        "        probabilities = softmax(outputs.logits, dim=1)\n",
        "        probabilities = probabilities.squeeze().tolist()  # If there's only one input sentence\n",
        "\n",
        "        # Pair each label with its corresponding probability\n",
        "        label_probabilities = zip(label_names, probabilities)\n",
        "\n",
        "        # Print the probabilities per label\n",
        "        for label, prob in label_probabilities:\n",
        "            print(f\"{label}: {prob:.4f}\")\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_set = DataSet()\n",
        "    my_model = ClassificationModel(data_set)\n",
        "    my_model.inference(INPUT)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1BMw9_zkRi7d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}